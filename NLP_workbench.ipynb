{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d39b89",
   "metadata": {},
   "source": [
    "# NLP Workbench (All-in-one) \n",
    "This consolidated notebook meets Documentation & Deliverables requirements and adds bonus features.\n",
    "\n",
    "## Submission Checklist\n",
    "- Functional .ipynb (this file)\n",
    "- Clear quick-start instructions\n",
    "- Rigorous evaluation (CV, macro-F1, confusion matrix, CI)\n",
    "- Visualizations (metrics & topics)\n",
    "- Report PDF export (≤10 pages)\n",
    "- Extra features: Prediction Explained, Batch Classify, Export CSV, caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f98e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages and libraries needed\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, simpledialog, ttk\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PyPDF2 import PdfReader\n",
    "import docx as docx_mod\n",
    "import joblib\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "\n",
    "# Conditional ReportLab import\n",
    "try:\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.lib.styles import getSampleStyleSheet\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
    "    from reportlab.lib import colors\n",
    "    REPORTLAB_AVAILABLE = True\n",
    "except Exception:\n",
    "    REPORTLAB_AVAILABLE = False\n",
    "# ------------------------\n",
    "# Resources\n",
    "# -------------------------\n",
    "def ensure_nltk_resources() -> None:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/stopwords\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"stopwords\", quiet=True)\n",
    "    try:\n",
    "        nltk.data.find(\"corpora/wordnet\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"wordnet\", quiet=True)\n",
    "    try:\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "ensure_nltk_resources()\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "# -------------------------\n",
    "# Classifier wrapper\n",
    "# -------------------------\n",
    "ARXIV_DATASET_ID = \"ccdv/arxiv-classification\"\n",
    "MODEL_CACHE = \"arxiv_textclf.joblib\"\n",
    "class ArxivTextClassifier:\n",
    "    def __init__(self) -> None:\n",
    "        self.pipeline: Pipeline | None = None\n",
    "        self.label_names: List[str] | None = None\n",
    "        self.report_text: str = \"\"\n",
    "        self.test_f1_per_class: np.ndarray | None = None\n",
    "        self.test_cm: np.ndarray | None = None\n",
    "        \n",
    "    def _build_pipeline(self, max_features: int = 100_000) -> Pipeline:\n",
    "        tfidf = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words=\"english\",\n",
    "            strip_accents=\"ascii\",\n",
    "            max_df=0.7,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=3,\n",
    "            sublinear_tf=True,\n",
    "        )\n",
    "        clf = SGDClassifier(\n",
    "            loss=\"hinge\",\n",
    "            alpha=1e-5,\n",
    "            max_iter=2_000,\n",
    "            tol=1e-4,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42,\n",
    "        )\n",
    "        return Pipeline([(\"tfidf\", tfidf), (\"clf\", clf)])\n",
    "\n",
    "    def _bootstrap_acc_ci(self, y_true: np.ndarray, y_pred: np.ndarray, n_boot: int = 200, seed: int = 42) -> Tuple[float, float]:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        n = len(y_true)\n",
    "        accs = []\n",
    "        for _ in range(n_boot):\n",
    "            idx = rng.integers(0, n, n)\n",
    "            accs.append(accuracy_score(y_true[idx], y_pred[idx]))\n",
    "        low, high = np.percentile(accs, [2.5, 97.5])\n",
    "        return float(low), float(high)\n",
    "\n",
    "    def train(self, max_features: int = 100_000) -> None:\n",
    "        ds = load_dataset(ARXIV_DATASET_ID)\n",
    "        self.label_names = ds[\"train\"].features[\"label\"].names\n",
    "        train_df = ds[\"train\"].to_pandas()\n",
    "        test_df = ds[\"test\"].to_pandas()\n",
    "        X_train = train_df[\"text\"].astype(str).values\n",
    "        y_train = train_df[\"label\"].astype(int).values\n",
    "        X_test = test_df[\"text\"].astype(str).values\n",
    "        y_test = test_df[\"label\"].astype(int).values\n",
    "\n",
    "        pipeline = self._build_pipeline(max_features)\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_acc = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring=\"accuracy\")\n",
    "        cv_f1 = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring=\"f1_macro\")\n",
    "\n",
    "        self.pipeline = self._build_pipeline(max_features)\n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = self.pipeline.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        ci_low, ci_high = self._bootstrap_acc_ci(y_test, y_pred)\n",
    "        self.test_cm = cm\n",
    "        _, _, f1s, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "        self.test_f1_per_class = f1s\n",
    "\n",
    "        lines = []\n",
    "        lines.append(\"=== Cross-Validation (5-fold, train set) ===\")\n",
    "        lines.append(f\"Accuracy: {cv_acc.mean():.3f} ± {cv_acc.std():.3f}\")\n",
    "        lines.append(f\"Macro-F1: {cv_f1.mean():.3f} ± {cv_f1.std():.3f}\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"=== Hold-out Test (official split) ===\")\n",
    "        lines.append(f\"Accuracy: {acc:.3f} (95% bootstrap CI: {ci_low:.3f}–{ci_high:.3f})\")\n",
    "        lines.append(f\"Macro-F1: {f1m:.3f}\")\n",
    "        lines.append(\"Confusion Matrix (counts):\")\n",
    "        lines.append(str(cm))\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Classification Report:\")\n",
    "        lines.append(classification_report(y_test, y_pred))\n",
    "        self.report_text = \"\\n\".join(lines)\n",
    "\n",
    "    def predict_label(self, text: str) -> str:\n",
    "        if not self.pipeline or not self.label_names:\n",
    "            raise RuntimeError(\"Classifier not trained. Call train() first.\")\n",
    "        pred_idx = int(self.pipeline.predict([text])[0])\n",
    "        return self.label_names[pred_idx]\n",
    "\n",
    "    def explain(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:\n",
    "        tfidf: TfidfVectorizer = self.pipeline.named_steps[\"tfidf\"]\n",
    "        clf: SGDClassifier = self.pipeline.named_steps[\"clf\"]\n",
    "        vec = tfidf.transform([text])\n",
    "        pred_idx = int(clf.predict(vec)[0])\n",
    "        coefs = clf.coef_[pred_idx]\n",
    "        nz = vec.nonzero()\n",
    "        indices = nz[1]\n",
    "        data = vec.data\n",
    "        contribs = []\n",
    "        for col, val in zip(indices, data):\n",
    "            contribs.append((tfidf.get_feature_names_out()[col], float(val * coefs[col])))\n",
    "        contribs.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        return contribs[:top_k]\n",
    "\n",
    "# -------------------------\n",
    "# Text utilities\n",
    "# -------------------------\n",
    "\n",
    "def treebank_to_wordnet(tag: str) -> str:\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    if tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    tokens = [t for t in word_tokenize(text.lower()) if t.isalpha()]\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    lemmatized = [LEMMATIZER.lemmatize(w, treebank_to_wordnet(tag)) for w, tag in tagged]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "# -------------------------\n",
    "# GUI Application\n",
    "# -------------------------\n",
    "\n",
    "class NLPWorkbench:\n",
    "    TECH_TERMS = [\n",
    "        \"Python\", \"Java\", \"C\", \"C#\", \"C++\", \"SQL\", \"TypeScript\", \"JavaScript\",\n",
    "        \"HTML\", \"CSS\", \"PHP\",\n",
    "        \"NumPy\", \"Keras\", \"TensorFlow\", \"PyTorch\", \"OpenCV\", \"Librosa\",\n",
    "        \"Pandas\", \"sklearn\", \"YOLO\", \"matplotlib\",\n",
    "        \"Power BI\", \"Excel\", \"Excel Analytics\", \"SQL Server\", \"Microsoft SQL\", \"n8n\",\n",
    "        \"React\", \".NET\", \"Spring Boot\", \"Postman\", \"MongoDB\", \"MySQL\",\n",
    "        \"Azure\", \"Git\", \"GitHub\", \"Jira\", \"Word\", \"PowerPoint\", \"Teams\",\n",
    "        \"NLP\", \"Machine Learning\", \"Deep Learning\", \"CRUD\",\n",
    "        \"Object Oriented Programming\", \"Database\", \"Data Modeling\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root: tk.Tk) -> None:\n",
    "        self.root = root\n",
    "        self.root.title(\"NLP Workbench (All-in-one)\")\n",
    "        self._init_style()\n",
    "        self._init_layout()\n",
    "\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self._add_entity_ruler()\n",
    "\n",
    "        self.classifier = ArxivTextClassifier()\n",
    "        self._model_ready = False\n",
    "        self._training_thread: threading.Thread | None = None\n",
    "        self._train_error = None\n",
    "\n",
    "        self._lda_model: LatentDirichletAllocation | None = None\n",
    "        self._lda_vectorizer: CountVectorizer | None = None\n",
    "        self._lda_top_words: List[List[str]] | None = None\n",
    "\n",
    "        self._load_cached_model()\n",
    "        self._bind_shortcuts()\n",
    "\n",
    "    # ---------- UI ----------\n",
    "    def _init_style(self) -> None:\n",
    "        style = ttk.Style()\n",
    "        style.theme_use(\"clam\")\n",
    "        self.bg_color = \"#f4f4f5\"\n",
    "        self.panel_bg = \"#ffffff\"\n",
    "        self.accent_color = \"#2563eb\"\n",
    "        self.accent_dark = \"#1d4ed8\"\n",
    "        self.text_color = \"#111827\"\n",
    "        self.root.configure(bg=self.bg_color)\n",
    "        style.configure(\"TLabelFrame\", background=self.bg_color, borderwidth=1)\n",
    "        style.configure(\"TFrame\", background=self.bg_color)\n",
    "        style.configure(\"TLabel\", background=self.bg_color, foreground=self.text_color)\n",
    "        style.configure(\"Accent.TButton\", foreground=\"white\", background=self.accent_color)\n",
    "        style.map(\"Accent.TButton\", background=[(\"active\", self.accent_dark)])\n",
    "\n",
    "    def _init_layout(self) -> None:\n",
    "        default_font = (\"Segoe UI\", 10)\n",
    "        title_font = (\"Segoe UI\", 16, \"bold\")\n",
    "\n",
    "        menubar = tk.Menu(self.root)\n",
    "        file_menu = tk.Menu(menubar, tearoff=0)\n",
    "        file_menu.add_command(label=\"Load File\", command=self.load_file)\n",
    "        file_menu.add_separator()\n",
    "        file_menu.add_command(label=\"Export CSV\", command=self.export_csv)\n",
    "        file_menu.add_separator()\n",
    "        file_menu.add_command(label=\"Exit\", command=self.root.quit)\n",
    "        menubar.add_cascade(label=\"File\", menu=file_menu)\n",
    "\n",
    "        view_menu = tk.Menu(menubar, tearoff=0)\n",
    "        view_menu.add_command(label=\"Clear Input\", command=self.clear_input)\n",
    "        view_menu.add_command(label=\"Clear Output\", command=self.clear_output)\n",
    "        view_menu.add_command(label=\"Clear Input & Output\", command=self.clear_both)\n",
    "        menubar.add_cascade(label=\"View\", menu=view_menu)\n",
    "\n",
    "        help_menu = tk.Menu(menubar, tearoff=0)\n",
    "        help_menu.add_command(label=\"Quick Start\", command=self.quick_start)\n",
    "        help_menu.add_command(label=\"About\", command=self.show_about)\n",
    "        help_menu.add_command(label=\"Submission Checklist\", command=self.submission_checklist)\n",
    "        menubar.add_cascade(label=\"Help\", menu=help_menu)\n",
    "        self.root.config(menu=menubar)\n",
    "\n",
    "        main_frame = ttk.Frame(self.root, padding=10)\n",
    "        main_frame.pack(fill=\"both\", expand=True)\n",
    "        title_label = ttk.Label(main_frame, text=\"NLP Workbench\", font=title_font)\n",
    "        title_label.pack(anchor=\"center\", pady=(0, 6))\n",
    "        subtitle = ttk.Label(main_frame, text=(\n",
    "            \"Step 1: Load → Step 2: Preprocess/Sentiment/NER → Step 3: LDA or Train Classifier (bg) → Step 4: Classify/Visualize/Report\"\n",
    "        ), foreground=\"#6b7280\")\n",
    "        subtitle.pack(anchor=\"center\", pady=(0, 10))\n",
    "\n",
    "        io_frame = ttk.Frame(main_frame)\n",
    "        io_frame.pack(fill=\"both\", expand=True)\n",
    "\n",
    "        input_frame = ttk.LabelFrame(io_frame, text=\"Input Text\")\n",
    "        input_frame.pack(side=\"left\", fill=\"both\", expand=True, padx=(0, 5))\n",
    "        self.text_input = tk.Text(input_frame, height=18, wrap=\"word\", font=default_font)\n",
    "        self.text_input.pack(side=\"left\", fill=\"both\", expand=True, padx=5, pady=5)\n",
    "        self.text_input.configure(bg=self.panel_bg, fg=self.text_color, insertbackground=self.accent_color)\n",
    "        input_scroll = ttk.Scrollbar(input_frame, orient=\"vertical\", command=self.text_input.yview)\n",
    "        input_scroll.pack(side=\"right\", fill=\"y\")\n",
    "        self.text_input.configure(yscrollcommand=input_scroll.set)\n",
    "\n",
    "        output_frame = ttk.LabelFrame(io_frame, text=\"Output\")\n",
    "        output_frame.pack(side=\"left\", fill=\"both\", expand=True, padx=(5, 0))\n",
    "        self.output = tk.Text(output_frame, height=18, wrap=\"word\", font=default_font, state=\"normal\")\n",
    "        self.output.pack(side=\"left\", fill=\"both\", expand=True, padx=5, pady=5)\n",
    "        self.output.configure(bg=self.panel_bg, fg=self.text_color, insertbackground=self.accent_color)\n",
    "        output_scroll = ttk.Scrollbar(output_frame, orient=\"vertical\", command=self.output.yview)\n",
    "        output_scroll.pack(side=\"right\", fill=\"y\")\n",
    "        self.output.configure(yscrollcommand=output_scroll.set)\n",
    "\n",
    "        controls_frame = ttk.Frame(main_frame)\n",
    "        controls_frame.pack(fill=\"x\", pady=(10, 5))\n",
    "\n",
    "        file_frame = ttk.LabelFrame(controls_frame, text=\"Data\")\n",
    "        file_frame.pack(side=\"left\", padx=5, pady=5, fill=\"x\", expand=True)\n",
    "        btn_load = ttk.Button(file_frame, text=\"Load File\", command=self.load_file, style=\"Accent.TButton\")\n",
    "        btn_load.pack(side=\"left\", padx=5, pady=5)\n",
    "\n",
    "        docs_frame = ttk.LabelFrame(controls_frame, text=\"Docs & Export\")\n",
    "        docs_frame.pack(side=\"left\", padx=5, pady=5, fill=\"x\", expand=True)\n",
    "        # Button text and behavior depend on reportlab availability\n",
    "        report_btn_text = \"Save Report (PDF)\" if REPORTLAB_AVAILABLE else \"Save Report (.docx)\"\n",
    "        self.report_btn = ttk.Button(docs_frame, text=report_btn_text, command=self.save_report_auto, style=\"Accent.TButton\")\n",
    "        self.report_btn.pack(side=\"left\", padx=5, pady=5)\n",
    "        self.batch_btn = ttk.Button(docs_frame, text=\"Batch Classify (lines)\", command=self.batch_classify, style=\"Accent.TButton\")\n",
    "        self.batch_btn.pack(side=\"left\", padx=5, pady=5)\n",
    "\n",
    "        basic_frame = ttk.LabelFrame(controls_frame, text=\"Basic NLP\")\n",
    "        basic_frame.pack(side=\"left\", padx=5, pady=5, fill=\"x\", expand=True)\n",
    "        btn_pp = ttk.Button(basic_frame, text=\"Preprocess\", command=self.preprocess, style=\"Accent.TButton\")\n",
    "        btn_pp.pack(side=\"left\", padx=5, pady=5)\n",
    "        btn_sent = ttk.Button(basic_frame, text=\"Sentiment\", command=self.sentiment, style=\"Accent.TButton\")\n",
    "        btn_sent.pack(side=\"left\", padx=5, pady=5)\n",
    "        btn_ner = ttk.Button(basic_frame, text=\"NER\", command=self.ner, style=\"Accent.TButton\")\n",
    "        btn_ner.pack(side=\"left\", padx=5, pady=5)\n",
    "\n",
    "        model_frame = ttk.LabelFrame(controls_frame, text=\"Modeling\")\n",
    "        model_frame.pack(side=\"left\", padx=5, pady=5, fill=\"x\", expand=True)\n",
    "        btn_lda = ttk.Button(model_frame, text=\"Topic Modeling (LDA)\", command=self.lda, style=\"Accent.TButton\")\n",
    "        btn_lda.pack(side=\"left\", padx=5, pady=5)\n",
    "        self.train_button = ttk.Button(model_frame, text=\"Train Classifier (bg)\", command=self.train_classifier_async, style=\"Accent.TButton\")\n",
    "        self.train_button.pack(side=\"left\", padx=5, pady=5)\n",
    "        self.classify_button = ttk.Button(model_frame, text=\"Text Classification\", command=self.text_classify, style=\"Accent.TButton\")\n",
    "        self.classify_button.pack(side=\"left\", padx=5, pady=5)\n",
    "        self.report_button = ttk.Button(model_frame, text=\"Model Report\", command=self.model_report, style=\"Accent.TButton\")\n",
    "        self.report_button.pack(side=\"left\", padx=5, pady=5)\n",
    "        self.explain_button = ttk.Button(model_frame, text=\"Explain Prediction\", command=self.explain_prediction, style=\"Accent.TButton\")\n",
    "        self.explain_button.pack(side=\"left\", padx=5, pady=5)\n",
    "        self.visual_button = ttk.Button(model_frame, text=\"Visualize Metrics\", command=self.visualize_metrics, style=\"Accent.TButton\")\n",
    "        self.visual_button.pack(side=\"left\", padx=5, pady=5)\n",
    "        self.topic_vis_button = ttk.Button(model_frame, text=\"Visualize Topics\", command=self.visualize_topics, style=\"Accent.TButton\")\n",
    "        self.topic_vis_button.pack(side=\"left\", padx=5, pady=5)\n",
    "\n",
    "        self.status_var = tk.StringVar(value=\"Ready\")\n",
    "        status_bar = ttk.Label(self.root, textvariable=self.status_var, relief=\"sunken\", anchor=\"w\", padding=(5, 2))\n",
    "        status_bar.pack(side=\"bottom\", fill=\"x\")\n",
    "        self.progress = ttk.Progressbar(self.root, mode=\"indeterminate\")\n",
    "        self.progress.pack(side=\"bottom\", fill=\"x\")\n",
    "        self.progress.stop()\n",
    "\n",
    "        self._set_classify_enabled(False)\n",
    "\n",
    "    def _add_entity_ruler(self) -> None:\n",
    "        ruler = self.nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"phrase_matcher_attr\": \"LOWER\"})\n",
    "        patterns = [{\"label\": \"TECH\", \"pattern\": term} for term in self.TECH_TERMS]\n",
    "        ruler.add_patterns(patterns)\n",
    "\n",
    "    def _bind_shortcuts(self) -> None:\n",
    "        self.root.bind(\"<Control-l>\", lambda e: self.load_file())\n",
    "        self.root.bind(\"<Control-p>\", lambda e: self.preprocess())\n",
    "        self.root.bind(\"<Control-s>\", lambda e: self.sentiment())\n",
    "        self.root.bind(\"<Control-n>\", lambda e: self.ner())\n",
    "        self.root.bind(\"<Control-t>\", lambda e: self.lda())\n",
    "        self.root.bind(\"<Control-r>\", lambda e: self.train_classifier_async())\n",
    "        self.root.bind(\"<Control-c>\", lambda e: self.text_classify())\n",
    "        self.root.bind(\"<Control-m>\", lambda e: self.model_report())\n",
    "        self.root.bind(\"<Control-e>\", lambda e: self.explain_prediction())\n",
    "        self.root.bind(\"<Control-v>\", lambda e: self.visualize_metrics())\n",
    "        self.root.bind(\"<Control-Shift-v>\", lambda e: self.visualize_topics())\n",
    "        self.root.bind(\"<Control-b>\", lambda e: self.batch_classify())\n",
    "        self.root.bind(\"<Control-w>\", lambda e: self.save_report_auto())\n",
    "        self.root.bind(\"<Control-x>\", lambda e: self.export_csv())\n",
    "\n",
    "    # ---------- Helpers ----------\n",
    "    def _set_classify_enabled(self, enabled: bool) -> None:\n",
    "        state = \"normal\" if enabled else \"disabled\"\n",
    "        self.classify_button.configure(state=state)\n",
    "        self.explain_button.configure(state=state)\n",
    "\n",
    "    def _load_cached_model(self) -> None:\n",
    "        if os.path.exists(MODEL_CACHE):\n",
    "            try:\n",
    "                cache = joblib.load(MODEL_CACHE)\n",
    "                self.classifier.pipeline = cache.get(\"pipeline\")\n",
    "                self.classifier.label_names = cache.get(\"labels\")\n",
    "                self.classifier.report_text = cache.get(\"report\", \"\")\n",
    "                self.classifier.test_cm = cache.get(\"cm\", None)\n",
    "                self.classifier.test_f1_per_class = cache.get(\"f1s\", None)\n",
    "                if self.classifier.pipeline and self.classifier.label_names:\n",
    "                    self._model_ready = True\n",
    "                    self._set_classify_enabled(True)\n",
    "                    self.update_status(\"Loaded cached classifier.\")\n",
    "            except Exception as exc:\n",
    "                self.update_status(f\"Failed to load cache: {exc}\")\n",
    "\n",
    "    def _save_cached_model(self) -> None:\n",
    "        if self.classifier.pipeline and self.classifier.label_names:\n",
    "            joblib.dump({\n",
    "                \"pipeline\": self.classifier.pipeline,\n",
    "                \"labels\": self.classifier.label_names,\n",
    "                \"report\": self.classifier.report_text,\n",
    "                \"cm\": self.classifier.test_cm,\n",
    "                \"f1s\": self.classifier.test_f1_per_class,\n",
    "            }, MODEL_CACHE)\n",
    "\n",
    "    def update_status(self, message: str) -> None:\n",
    "        self.status_var.set(message)\n",
    "        self.root.update_idletasks()\n",
    "\n",
    "    def clear_input(self) -> None:\n",
    "        self.text_input.delete(\"1.0\", tk.END)\n",
    "        self.update_status(\"Input cleared.\")\n",
    "\n",
    "    def clear_output(self) -> None:\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.update_status(\"Output cleared.\")\n",
    "\n",
    "    def clear_both(self) -> None:\n",
    "        self.text_input.delete(\"1.0\", tk.END)\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.update_status(\"Input and output cleared.\")\n",
    "\n",
    "    def show_about(self) -> None:\n",
    "        messagebox.showinfo(\n",
    "            \"About\",\n",
    "            \"NLP Workbench (All-in-one)\\n\\n\"\n",
    "            \"• Async training + caching\\n\"\n",
    "            \"• Rigorous evaluation (CV, macro-F1, CI)\\n\"\n",
    "            \"• Visualizations & Explainability\\n\"\n",
    "            \"• Docs: report export & submission checklist\"\n",
    "        )\n",
    "\n",
    "    def quick_start(self) -> None:\n",
    "        msg = (\n",
    "            \"Quick Start:\\n\\n\"\n",
    "            \"1) Load file (Ctrl+L)\\n\"\n",
    "            \"2) Preprocess (Ctrl+P), Sentiment (Ctrl+S), NER (Ctrl+N)\\n\"\n",
    "            \"3) LDA (Ctrl+T) or Train Classifier (Ctrl+R)\\n\"\n",
    "            \"4) Classify (Ctrl+C); Visualize Metrics (Ctrl+V); Explain (Ctrl+E)\\n\"\n",
    "            \"5) Batch Classify (Ctrl+B); Export CSV (Ctrl+X); Save Report (Ctrl+W)\\n\"\n",
    "        )\n",
    "        messagebox.showinfo(\"Quick Start\", msg)\n",
    "\n",
    "    def submission_checklist(self) -> None:\n",
    "        msg = (\n",
    "            \"Submission Checklist:\\n\\n\"\n",
    "            \"• Functional notebook (.ipynb) — this all-in-one build\\n\"\n",
    "            \"• Code quality: PEP8, documentation, error handling\\n\"\n",
    "            \"• Evaluation: CV + test metrics, confusion matrix, macro-F1, CI\\n\"\n",
    "            \"• Visualizations: metrics & topics\\n\"\n",
    "            \"• Report (PDF or DOCX ≤ 10 pages): use 'Save Report'\\n\"\n",
    "            \"• Extra includes: Explain Prediction, Batch Classify, Export CSV\\n\"\n",
    "        )\n",
    "        messagebox.showinfo(\"Submission Checklist\", msg)\n",
    "\n",
    "    # ---------- File loading ----------\n",
    "    def load_file(self) -> None:\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select a file\",\n",
    "            filetypes=[\n",
    "                (\"Text files\", \"*.txt\"),\n",
    "                (\"Word documents\", \"*.docx\"),\n",
    "                (\"PDF files\", \"*.pdf\"),\n",
    "                (\"Excel files\", \"*.xlsx *.xls\"),\n",
    "                (\"All files\", \".\"),\n",
    "            ],\n",
    "        )\n",
    "        if not file_path:\n",
    "            return\n",
    "        try:\n",
    "            content = self._read_file_content(file_path)\n",
    "            self.text_input.delete(\"1.0\", tk.END)\n",
    "            self.text_input.insert(tk.END, content)\n",
    "            self.update_status(f\"Loaded file: {file_path}\")\n",
    "        except Exception as exc:\n",
    "            messagebox.showerror(\"Error\", f\"Could not read file:\\n{exc}\")\n",
    "            self.update_status(\"Failed to load file.\")\n",
    "\n",
    "    def _read_file_content(self, file_path: str) -> str:\n",
    "        lower = file_path.lower()\n",
    "        if lower.endswith(\".txt\"):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                return f.read()\n",
    "        if lower.endswith(\".docx\"):\n",
    "            document = docx_mod.Document(file_path)\n",
    "            return \"\\n\".join(p.text for p in document.paragraphs if p.text.strip())\n",
    "        if lower.endswith(\".pdf\"):\n",
    "            reader = PdfReader(file_path)\n",
    "            pages_text: List[str] = []\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    pages_text.append(text.strip())\n",
    "            return \"\\n\\n\".join(pages_text)\n",
    "        if lower.endswith((\".xlsx\", \".xls\")):\n",
    "            engine = \"openpyxl\" if lower.endswith(\".xlsx\") else \"xlrd\"\n",
    "            df = pd.read_excel(file_path, engine=engine)\n",
    "            rows_as_text: List[str] = []\n",
    "            for row in df.itertuples(index=False):\n",
    "                cells = [str(v) for v in row if pd.notna(v)]\n",
    "                if cells:\n",
    "                    rows_as_text.append(\" \".join(cells))\n",
    "            return \"\\n\".join(rows_as_text) if rows_as_text else \"\"\n",
    "        raise ValueError(\"Unsupported file type. Use .txt, .pdf, .docx, .xlsx, or .xls.\")\n",
    "\n",
    "    # ---------- NLP actions ----------\n",
    "    def preprocess(self) -> None:\n",
    "        text = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        if not text:\n",
    "            self.update_status(\"No input to preprocess.\")\n",
    "            return\n",
    "        processed = preprocess_text(text)\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, processed)\n",
    "        self.update_status(\"Preprocessing completed!\")\n",
    "\n",
    "    def sentiment(self) -> None:\n",
    "        text = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        if not text:\n",
    "            self.update_status(\"No input for sentiment analysis.\")\n",
    "            return\n",
    "        blob = TextBlob(text)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        if polarity > 0:\n",
    "            sentiment = f\"Positive ({polarity:.2f})\"\n",
    "            advice = \"Highlight strengths or positive cues.\"\n",
    "        elif polarity < 0:\n",
    "            sentiment = f\"Negative ({polarity:.2f})\"\n",
    "            advice = \"Address concerns or clarify issues.\"\n",
    "        else:\n",
    "            sentiment = \"Neutral\"\n",
    "            advice = \"Add specific details or examples for clarity.\"\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, sentiment + \"\\n\" + advice)\n",
    "        self.update_status(\"Sentiment analysis completed!\")\n",
    "\n",
    "    def ner(self) -> None:\n",
    "        text = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        if not text:\n",
    "            self.update_status(\"No input for NER.\")\n",
    "            return\n",
    "        doc = self.nlp(text)\n",
    "        results = \"\\n\".join(f\"{ent.text} — {ent.label_}\" for ent in doc.ents)\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, results)\n",
    "        self.update_status(f\"NER completed with {len(doc.ents)} entities.\")\n",
    "\n",
    "    def _topic_modeling(self, docs: Iterable[str], num_topics: int = 4, num_words: int = 5) -> str:\n",
    "        processed_docs = [preprocess_text(doc) for doc in docs]\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(processed_docs)\n",
    "        lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "        lda_model.fit(X)\n",
    "        words = vectorizer.get_feature_names_out()\n",
    "        lines = []\n",
    "        top_words_all: List[List[str]] = []\n",
    "        for idx, topic in enumerate(lda_model.components_):\n",
    "            top_indices = topic.argsort()[:-num_words - 1:-1]\n",
    "            top_words = [words[i] for i in top_indices]\n",
    "            top_words_all.append(top_words)\n",
    "            lines.append(f\"Topic {idx + 1}: {', '.join(top_words)}\")\n",
    "        perplex = lda_model.perplexity(X)\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"Perplexity (lower is better): {perplex:.2f}\")\n",
    "        self._lda_model = lda_model\n",
    "        self._lda_vectorizer = vectorizer\n",
    "        self._lda_top_words = top_words_all\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def lda(self) -> None:\n",
    "        raw_text = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        if not raw_text:\n",
    "            self.update_status(\"No input for LDA.\")\n",
    "            return\n",
    "        docs = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        if not docs:\n",
    "            self.update_status(\"No documents found for LDA.\")\n",
    "            return\n",
    "        max_docs = simpledialog.askinteger(\"Number of Documents\", f\"How many documents to use? (max {len(docs)})\", minvalue=1, maxvalue=len(docs), parent=self.root)\n",
    "        if max_docs is None:\n",
    "            return\n",
    "        docs = docs[:max_docs]\n",
    "        num_topics = simpledialog.askinteger(\"Number of Topics\", \"Enter number of topics:\", minvalue=1, maxvalue=10, parent=self.root)\n",
    "        if num_topics is None:\n",
    "            return\n",
    "        num_words = simpledialog.askinteger(\"Words per Topic\", \"Enter number of words per topic:\", minvalue=1, maxvalue=20, parent=self.root)\n",
    "        if num_words is None:\n",
    "            return\n",
    "        result = self._topic_modeling(docs, num_topics=num_topics, num_words=num_words)\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, result)\n",
    "        self.update_status(f\"LDA completed with {num_topics} topics on {len(docs)} documents.\")\n",
    "\n",
    "    # ---------- Classification ----------\n",
    "    def train_classifier_async(self) -> None:\n",
    "        if self._training_thread and self._training_thread.is_alive():\n",
    "            self.update_status(\"Training already in progress…\")\n",
    "            return\n",
    "        self.update_status(\"Training classifier in background (CV + test)…\")\n",
    "        self.progress.start(10)\n",
    "        self.train_button.configure(state=\"disabled\")\n",
    "        self._training_thread = threading.Thread(target=self._train_worker, daemon=True)\n",
    "        self._training_thread.start()\n",
    "        self.root.after(250, self._check_training_done)\n",
    "\n",
    "    def _train_worker(self) -> None:\n",
    "        try:\n",
    "            self.classifier.train()\n",
    "            self._model_ready = True\n",
    "            self._save_cached_model()\n",
    "        except Exception as exc:\n",
    "            self._train_error = exc\n",
    "        else:\n",
    "            self._train_error = None\n",
    "\n",
    "    def _check_training_done(self) -> None:\n",
    "        if self._training_thread and self._training_thread.is_alive():\n",
    "            self.root.after(250, self._check_training_done)\n",
    "            return\n",
    "        self.progress.stop()\n",
    "        self.train_button.configure(state=\"normal\")\n",
    "        if self._train_error:\n",
    "            messagebox.showerror(\"Training Error\", str(self._train_error))\n",
    "            self.update_status(\"Classifier training failed.\")\n",
    "        else:\n",
    "            self._set_classify_enabled(True)\n",
    "            self.update_status(\"Classifier trained (cached + evaluated). Use 'Model Report'.\")\n",
    "\n",
    "    def text_classify(self) -> None:\n",
    "        if not self._model_ready:\n",
    "            self.update_status(\"Classifier not ready yet.\")\n",
    "            return\n",
    "        text = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        if not text:\n",
    "            self.update_status(\"No input text to classify.\")\n",
    "            return\n",
    "        try:\n",
    "            result = self.classifier.predict_label(text)\n",
    "        except Exception as exc:\n",
    "            messagebox.showerror(\"Classification Error\", str(exc))\n",
    "            self.update_status(\"Classification failed.\")\n",
    "            return\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, result)\n",
    "        self.update_status(f\"Text classified as: {result}\")\n",
    "\n",
    "    def batch_classify(self) -> None:\n",
    "        if not self._model_ready:\n",
    "            self.update_status(\"Classifier not ready yet.\")\n",
    "            return\n",
    "        raw = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        docs = [ln.strip() for ln in raw.split(\"\\n\") if ln.strip()]\n",
    "        if not docs:\n",
    "            self.update_status(\"No lines to classify.\")\n",
    "            return\n",
    "        preds = []\n",
    "        for d in docs:\n",
    "            try:\n",
    "                preds.append(self.classifier.predict_label(d))\n",
    "            except Exception as exc:\n",
    "                preds.append(f\"ERROR: {exc}\")\n",
    "        out = \"\\n\".join(f\"{i+1}. {p}\" for i, p in enumerate(preds))\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, out)\n",
    "        self.update_status(f\"Batch classified {len(docs)} lines.\")\n",
    "\n",
    "    def export_csv(self) -> None:\n",
    "        if not self._model_ready:\n",
    "            self.update_status(\"Classifier not ready yet.\")\n",
    "            return\n",
    "        raw = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        docs = [ln.strip() for ln in raw.split(\"\\n\") if ln.strip()]\n",
    "        if not docs:\n",
    "            self.update_status(\"No lines to export.\")\n",
    "            return\n",
    "        preds = []\n",
    "        for d in docs:\n",
    "            try:\n",
    "                preds.append(self.classifier.predict_label(d))\n",
    "            except Exception as exc:\n",
    "                preds.append(f\"ERROR: {exc}\")\n",
    "        df = pd.DataFrame({\"text\": docs, \"prediction\": preds})\n",
    "        save_path = filedialog.asksaveasfilename(defaultextension=\".csv\", filetypes=[(\"CSV\", \"*.csv\")], title=\"Save predictions CSV\")\n",
    "        if not save_path:\n",
    "            return\n",
    "        try:\n",
    "            df.to_csv(save_path, index=False)\n",
    "            self.update_status(f\"Saved CSV to {save_path}\")\n",
    "        except Exception as exc:\n",
    "            messagebox.showerror(\"Export Error\", str(exc))\n",
    "            self.update_status(\"CSV export failed.\")\n",
    "\n",
    "    def explain_prediction(self) -> None:\n",
    "        if not self._model_ready:\n",
    "            self.update_status(\"Classifier not ready yet.\")\n",
    "            return\n",
    "        text = self.text_input.get(\"1.0\", tk.END).strip()\n",
    "        if not text:\n",
    "            self.update_status(\"No input to explain.\")\n",
    "            return\n",
    "        try:\n",
    "            contribs = self.classifier.explain(text, top_k=10)\n",
    "        except Exception as exc:\n",
    "            messagebox.showerror(\"Explain Error\", str(exc))\n",
    "            self.update_status(\"Explain failed.\")\n",
    "            return\n",
    "        lines = [\"Top contributing tokens (feature × weight):\"]\n",
    "        for tok, val in contribs:\n",
    "            lines.append(f\"{tok}: {val:.4f}\")\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, \"\\n\".join(lines))\n",
    "        self.update_status(\"Explanation generated.\")\n",
    "\n",
    "    def model_report(self) -> None:\n",
    "        if not self._model_ready:\n",
    "            self.update_status(\"Classifier not ready yet.\")\n",
    "            return\n",
    "        report = self.classifier.report_text or \"No report available. Train the classifier first.\"\n",
    "        self.output.delete(\"1.0\", tk.END)\n",
    "        self.output.insert(tk.END, report)\n",
    "        self.update_status(\"Model report displayed.\")\n",
    "\n",
    "    # ---------- Visualizations ----------\n",
    "    def visualize_metrics(self) -> None:\n",
    "        if not self._model_ready:\n",
    "            self.update_status(\"Train classifier first or load cache.\")\n",
    "            return\n",
    "        cm = self.classifier.test_cm\n",
    "        f1s = self.classifier.test_f1_per_class\n",
    "        labels = self.classifier.label_names\n",
    "        if cm is None or f1s is None or labels is None:\n",
    "            self.update_status(\"Metrics not available.\")\n",
    "            return\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        ax = axes[0]\n",
    "        im = ax.imshow(cm, cmap=\"Blues\")\n",
    "        ax.set_title(\"Confusion Matrix\")\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"True\")\n",
    "        ax.set_xticks(np.arange(len(labels)))\n",
    "        ax.set_yticks(np.arange(len(labels)))\n",
    "        ax.set_xticklabels(range(len(labels)))\n",
    "        ax.set_yticklabels(range(len(labels)))\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        ax2 = axes[1]\n",
    "        ax2.bar(np.arange(len(labels)), f1s, color=\"#2563eb\")\n",
    "        ax2.set_title(\"Per-class F1 (macro view)\")\n",
    "        ax2.set_xlabel(\"Class index\")\n",
    "        ax2.set_ylabel(\"F1 score\")\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "        fig.tight_layout()\n",
    "        win = tk.Toplevel(self.root)\n",
    "        win.title(\"Model Metrics\")\n",
    "        canvas = FigureCanvasTkAgg(fig, master=win)\n",
    "        canvas.draw()\n",
    "        canvas.get_tk_widget().pack(fill=\"both\", expand=True)\n",
    "        self.update_status(\"Metrics visualized.\")\n",
    "\n",
    "    def visualize_topics(self) -> None:\n",
    "        if not self._lda_model or not self._lda_vectorizer:\n",
    "            self.update_status(\"Run LDA first to visualize topics.\")\n",
    "            return\n",
    "        num_topics = self._lda_model.n_components\n",
    "        topic_idx = simpledialog.askinteger(\"Visualize Topic\", f\"Enter topic number (1–{num_topics})\", minvalue=1, maxvalue=num_topics, parent=self.root)\n",
    "        if topic_idx is None:\n",
    "            return\n",
    "        topic = self._lda_model.components_[topic_idx - 1]\n",
    "        words = self._lda_vectorizer.get_feature_names_out()\n",
    "        top_idx = topic.argsort()[:-10 - 1:-1]\n",
    "        top_words = [words[i] for i in top_idx]\n",
    "        top_vals = topic[top_idx]\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.barh(top_words[::-1], top_vals[::-1], color=\"#10b981\")\n",
    "        ax.set_title(f\"Topic {topic_idx}: Top words (weights)\")\n",
    "        ax.set_xlabel(\"Weight\")\n",
    "        ax.set_ylabel(\"Word\")\n",
    "        ax.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.4)\n",
    "        fig.tight_layout()\n",
    "        win = tk.Toplevel(self.root)\n",
    "        win.title(\"Topic Visualization\")\n",
    "        canvas = FigureCanvasTkAgg(fig, master=win)\n",
    "        canvas.draw()\n",
    "        canvas.get_tk_widget().pack(fill=\"both\", expand=True)\n",
    "        self.update_status(\"Topic visualized.\")\n",
    "\n",
    "    # ---------- Report (Auto: PDF or DOCX) ----------\n",
    "    def save_report_auto(self) -> None:\n",
    "        if not self._model_ready or not self.classifier.report_text:\n",
    "            self.update_status(\"Train classifier first to generate a report.\")\n",
    "            return\n",
    "        if REPORTLAB_AVAILABLE:\n",
    "            self._save_report_pdf()\n",
    "        else:\n",
    "            self._save_report_docx()\n",
    "\n",
    "    def _save_report_pdf(self) -> None:\n",
    "        try:\n",
    "            save_path = filedialog.asksaveasfilename(defaultextension=\".pdf\", filetypes=[(\"PDF\", \"*.pdf\")], title=\"Save Report PDF\")\n",
    "            if not save_path:\n",
    "                return\n",
    "            styles = getSampleStyleSheet()\n",
    "            doc = SimpleDocTemplate(save_path, pagesize=A4)\n",
    "            story = []\n",
    "            story.append(Paragraph(\"NLP Workbench — Submission Report\", styles['Title']))\n",
    "            story.append(Spacer(1, 12))\n",
    "            story.append(Paragraph(\"Author: Student\", styles['Normal']))\n",
    "            story.append(Paragraph(\"Tool: NLP Workbench (All-in-one)\", styles['Normal']))\n",
    "            story.append(Spacer(1, 12))\n",
    "            story.append(Paragraph(\"1. Overview\", styles['Heading2']))\n",
    "            story.append(Paragraph(\"This report summarizes the dataset, methodology, evaluation metrics, and UI/UX features. The classifier is TF–IDF + linear SVM (SGD) with stratified 5-fold CV. Topics are extracted via LDA.\", styles['Normal']))\n",
    "            story.append(Spacer(1, 8))\n",
    "            story.append(Paragraph(\"2. Methodology & Evaluation\", styles['Heading2']))\n",
    "            for line in self.classifier.report_text.split(\"\\n\"):\n",
    "                story.append(Paragraph(line, styles['Code']))\n",
    "            story.append(Spacer(1, 8))\n",
    "            story.append(Paragraph(\"3. UI/UX & Deliverables\", styles['Heading2']))\n",
    "            bullets = [\n",
    "                \"Responsive GUI (async training, progress bar)\",\n",
    "                \"Workflow cues, tooltips, keyboard shortcuts\",\n",
    "                \"Visualizations: confusion matrix, per-class F1, topic bars\",\n",
    "                \"Explain Prediction (top contributing tokens)\",\n",
    "                \"Batch classify + CSV export\",\n",
    "                \"Report export\",\n",
    "            ]\n",
    "            tbl = Table([[b] for b in bullets])\n",
    "            tbl.setStyle(TableStyle([\n",
    "                ('BACKGROUND', (0,0), (-1,-1), colors.whitesmoke),\n",
    "                ('TEXTCOLOR', (0,0), (-1,-1), colors.black),\n",
    "                ('FONTNAME', (0,0), (-1,-1), 'Helvetica'),\n",
    "                ('FONTSIZE', (0,0), (-1,-1), 10),\n",
    "                ('LEFTPADDING', (0,0), (-1,-1), 6),\n",
    "            ]))\n",
    "            story.append(tbl)\n",
    "            story.append(Spacer(1, 8))\n",
    "            story.append(Paragraph(\"4. Insights & Recommendations\", styles['Heading2']))\n",
    "            story.append(Paragraph(\"Use per-class F1 and confusion hotspots to target categories with lower recall.\", styles['Normal']))\n",
    "            story.append(Spacer(1, 8))\n",
    "            story.append(Paragraph(\"5. Limitations & Future Work\", styles['Heading2']))\n",
    "            story.append(Paragraph(\"Model uses linear SVM without probability calibration.\", styles['Normal']))\n",
    "            doc.build(story)\n",
    "            self.update_status(f\"Saved report to {save_path}\")\n",
    "        except Exception as exc:\n",
    "            messagebox.showerror(\"Report Error\", str(exc))\n",
    "            self.update_status(\"Report generation failed.\")\n",
    "\n",
    "    def _save_report_docx(self) -> None:\n",
    "        try:\n",
    "            save_path = filedialog.asksaveasfilename(defaultextension=\".docx\", filetypes=[(\"Word Document\", \"*.docx\")], title=\"Save Report DOCX\")\n",
    "            if not save_path:\n",
    "                return\n",
    "            doc = docx_mod.Document()\n",
    "            doc.add_heading('NLP Workbench — Submission Report', 0)\n",
    "            doc.add_paragraph('Author: Student')\n",
    "            doc.add_paragraph('Tool: NLP Workbench (All-in-one)')\n",
    "            doc.add_heading('1. Overview', level=1)\n",
    "            doc.add_paragraph('This report summarizes the dataset, methodology, evaluation metrics, and UI/UX features. The classifier is TF–IDF + linear SVM (SGD) with stratified 5-fold CV. Topics are extracted via LDA.')\n",
    "            doc.add_heading('2. Methodology & Evaluation', level=1)\n",
    "            for line in self.classifier.report_text.split(\"\\n\"):\n",
    "                doc.add_paragraph(line)\n",
    "            doc.add_heading('3. UI/UX & Deliverables', level=1)\n",
    "            bullets = [\n",
    "                'Responsive GUI (async training, progress bar)',\n",
    "                'Workflow cues, tooltips, keyboard shortcuts',\n",
    "                'Visualizations: confusion matrix, per-class F1, topic bars',\n",
    "                'Explain Prediction (top contributing tokens)',\n",
    "                'Batch classify + CSV export',\n",
    "                'Report export',\n",
    "            ]\n",
    "            for b in bullets:\n",
    "                doc.add_paragraph(b, style='List Bullet')\n",
    "            doc.add_heading('4. Insights & Recommendations', level=1)\n",
    "            doc.add_paragraph('Use per-class F1 and confusion hotspots to target categories with lower recall.')\n",
    "            doc.add_heading('5. Limitations & Future Work', level=1)\n",
    "            doc.add_paragraph('Model uses linear SVM without probability calibration.')\n",
    "            doc.save(save_path)\n",
    "            self.update_status(f\"Saved report to {save_path}\")\n",
    "        except Exception as exc:\n",
    "            messagebox.showerror(\"Report Error\", str(exc))\n",
    "            self.update_status(\"DOCX report generation failed.\")\n",
    "\n",
    "# ---------- Entrypoint ----------\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = NLPWorkbench(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9fb9c5-9621-4346-af54-e992bdfc4497",
   "metadata": {},
   "source": [
    "Note: If `reportlab` is not installed, the app will automatically offer **Save Report (DOCX)** as a fallback. If available, it uses **Save Report (PDF)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
